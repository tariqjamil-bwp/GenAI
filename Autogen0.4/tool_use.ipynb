{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['llama-3.1-70b-versatile', 'llama3-groq-70b-8192-tool-use-preview', 'llama3-70b-8192', 'mixtral-8x7b-32768']\n",
      "llama-3.1-70b-versatile\n",
      "Successfully initialized client with model: llama-3.1-70b-versatile\n",
      "llama3-groq-70b-8192-tool-use-preview\n",
      "Successfully initialized client with model: llama3-groq-70b-8192-tool-use-preview\n",
      "llama3-70b-8192\n",
      "Successfully initialized client with model: llama3-70b-8192\n",
      "mixtral-8x7b-32768\n",
      "Successfully initialized client with model: mixtral-8x7b-32768\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.agents import ToolUseAssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat, StopMessageTermination\n",
    "from autogen_core.components.models import OpenAIChatCompletionClient\n",
    "from autogen_core.components.tools import FunctionTool\n",
    "\n",
    "async def get_weather(city: str) -> str:\n",
    "    return f\"The weather in {city} is 72 degrees and Sunny.\"\n",
    "\n",
    "\n",
    "from model import create_clients\n",
    "client = create_clients(0)[2]\n",
    "\n",
    "get_weather_tool = FunctionTool(get_weather, description=\"Get the weather for a city\")\n",
    "\n",
    "assistant = ToolUseAssistantAgent(\n",
    "    \"Weather_Assistant\",\n",
    "    model_client=client,\n",
    "    registered_tools=[get_weather_tool],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "team = RoundRobinGroupChat([assistant])\n",
    "result = await team.run(\"What's the weather in New York?\", termination_condition=StopMessageTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TeamRunResult(messages=[TextMessage(source='user', content=\"What's the weather in New York?\"), StopMessage(source='Weather_Assistant', content='The weather in New York is 72 degrees and Sunny. TERMINATE')])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The weather in New York is 72 degrees and Sunny. TERMINATE'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.messages[1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain, langchain-community, wikipedia , autogen-ext\n",
    "\n",
    "import wikipedia\n",
    "from autogen_ext.tools.langchain import LangChainToolAdapter\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(wiki_client=wikipedia, top_k_results=1, doc_content_chars_max=100)\n",
    "tool = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "\n",
    "langchain_wikipedia_tool = LangChainToolAdapter(tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_assistant = ToolUseAssistantAgent(\n",
    "    \"WikiPedia_Assistant\",\n",
    "    model_client=client,\n",
    "    registered_tools=[langchain_wikipedia_tool],\n",
    ")\n",
    "team = RoundRobinGroupChat([wikipedia_assistant])\n",
    "result = await team.run(\n",
    "    \"Who was the first president of the United States?\", termination_condition=StopMessageTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'George Washington was the first president of the United States.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.messages[1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tjamil/Dev/GenAI/Autogen0.4/autogen_core/application/_single_threaded_agent_runtime.py\", line 385, in _process_publish\n",
      "    _all_responses = await asyncio.gather(*responses)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tjamil/Dev/GenAI/Autogen0.4/autogen_core/application/_single_threaded_agent_runtime.py\", line 376, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tjamil/.local/share/virtualenvs/Dev-A_vYTw5b/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 49, in on_message\n",
      "    return await super().on_message(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tjamil/Dev/GenAI/Autogen0.4/autogen_core/components/_routed_agent.py\", line 468, in on_message\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tjamil/Dev/GenAI/Autogen0.4/autogen_core/components/_routed_agent.py\", line 267, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tjamil/.local/share/virtualenvs/Dev-A_vYTw5b/lib/python3.11/site-packages/autogen_agentchat/teams/_group_chat/_base_chat_agent_container.py\", line 50, in handle_content_request\n",
      "    response = await self._agent.on_messages(self._message_buffer, ctx.cancellation_token)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tjamil/.local/share/virtualenvs/Dev-A_vYTw5b/lib/python3.11/site-packages/autogen_agentchat/agents/_code_executor_agent.py\", line 30, in on_messages\n",
      "    result = await self._code_executor.execute_code_blocks(code_blocks, cancellation_token=cancellation_token)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tjamil/Dev/GenAI/Autogen0.4/autogen_ext/code_executor/docker_executor/_impl.py\", line 278, in execute_code_blocks\n",
      "    return await self._execute_code_dont_check_setup(code_blocks, cancellation_token)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tjamil/Dev/GenAI/Autogen0.4/autogen_ext/code_executor/docker_executor/_impl.py\", line 243, in _execute_code_dont_check_setup\n",
      "    command = [\"timeout\", str(self._timeout), lang_to_cmd(lang), filename]\n",
      "                                              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tjamil/Dev/GenAI/Autogen0.4/autogen_core/components/code_executor/_impl/utils.py\", line 78, in lang_to_cmd\n",
      "    raise ValueError(f\"Unsupported language: {lang}\")\n",
      "ValueError: Unsupported language: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TeamRunResult(messages=[TextMessage(source='user', content=\"Create a plot of NVDIA and TSLA stock returns YTD from 2024-01-01 and save it to 'nvidia_tesla_2024_ytd.png'.\"), TextMessage(source='coding_assistant', content=\"I'd be happy to help you with that. To create a plot of NVDIA and TSLA stock returns YTD from 2024-01-01, I'll need to use Python with the necessary libraries.\\n\\nHere's the Python code to accomplish this task:\\n```\\n# filename: stock_returns_plot.py\\nimport yfinance as yf\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\n\\n# Define the stock tickers and the start date\\ntickers = ['NVIDIA', 'Tesla']\\nstart_date = '2024-01-01'\\n\\n# Download the data\\ndata = yf.download(tickers, start=start_date)['Adj Close']\\n\\n# Calculate the daily returns\\nreturns = data.pct_change()\\n\\n# Plot the returns\\nplt.figure(figsize=(12, 6))\\nfor ticker in tickers:\\n    plt.plot(returns[ticker], label=ticker)\\nplt.title('NVIDIA and Tesla Stock Returns YTD 2024')\\nplt.xlabel('Date')\\nplt.ylabel('Returns')\\nplt.legend()\\nplt.savefig('nvidia_tesla_2024_ytd.png')\\nplt.show()\\n```\\nPlease save this code in a file named `stock_returns_plot.py` and execute it. The script will download the necessary data, calculate the daily returns, and create a plot of the returns for NVDIA and TSLA stocks YTD from 2024-01-01. The plot will be saved as `nvidia_tesla_2024_ytd.png` in the same directory.\\n\\nOnce you execute the code, I'll be waiting for your response. If there are any errors or issues, I'll be happy to help you troubleshoot.\")])\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.agents import CodeExecutorAgent, CodingAssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat, StopMessageTermination\n",
    "from autogen_core.components.models import OpenAIChatCompletionClient\n",
    "from autogen_ext.code_executor.docker_executor import DockerCommandLineCodeExecutor\n",
    "\n",
    "async with DockerCommandLineCodeExecutor(work_dir=\"coding\") as code_executor:  # type: ignore[syntax]\n",
    "    code_executor_agent = CodeExecutorAgent(\"code_executor\", code_executor=code_executor)\n",
    "    coding_assistant_agent = CodingAssistantAgent(\n",
    "        \"coding_assistant\", model_client=client,\n",
    "    )\n",
    "    group_chat = RoundRobinGroupChat([coding_assistant_agent, code_executor_agent])\n",
    "    result = await group_chat.run(\n",
    "        task=\"Create a plot of NVDIA and TSLA stock returns YTD from 2024-01-01 and save it to 'nvidia_tesla_2024_ytd.png'.\",\n",
    "        termination_condition=StopMessageTermination(),\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'd be happy to help you with that. To create a plot of NVDIA and TSLA stock returns YTD from 2024-01-01, I'll need to use Python with the necessary libraries.\\n\\nHere's the Python code to accomplish this task:\\n```\\n# filename: stock_returns_plot.py\\nimport yfinance as yf\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\n\\n# Define the stock tickers and the start date\\ntickers = ['NVIDIA', 'Tesla']\\nstart_date = '2024-01-01'\\n\\n# Download the data\\ndata = yf.download(tickers, start=start_date)['Adj Close']\\n\\n# Calculate the daily returns\\nreturns = data.pct_change()\\n\\n# Plot the returns\\nplt.figure(figsize=(12, 6))\\nfor ticker in tickers:\\n    plt.plot(returns[ticker], label=ticker)\\nplt.title('NVIDIA and Tesla Stock Returns YTD 2024')\\nplt.xlabel('Date')\\nplt.ylabel('Returns')\\nplt.legend()\\nplt.savefig('nvidia_tesla_2024_ytd.png')\\nplt.show()\\n```\\nPlease save this code in a file named `stock_returns_plot.py` and execute it. The script will download the necessary data, calculate the daily returns, and create a plot of the returns for NVDIA and TSLA stocks YTD from 2024-01-01. The plot will be saved as `nvidia_tesla_2024_ytd.png` in the same directory.\\n\\nOnce you execute the code, I'll be waiting for your response. If there are any errors or issues, I'll be happy to help you troubleshoot.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.messages[1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'coding/nvidia_tesla_2024_ytd.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m----> 3\u001b[0m \u001b[43mImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoding/nvidia_tesla_2024_ytd.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Dev-A_vYTw5b/lib/python3.11/site-packages/IPython/core/display.py:974\u001b[0m, in \u001b[0;36mImage.__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munconfined \u001b[38;5;241m=\u001b[39m unconfined\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malt \u001b[38;5;241m=\u001b[39m alt\n\u001b[0;32m--> 974\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, {}):\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Dev-A_vYTw5b/lib/python3.11/site-packages/IPython/core/display.py:331\u001b[0m, in \u001b[0;36mDisplayObject.__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_data()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Dev-A_vYTw5b/lib/python3.11/site-packages/IPython/core/display.py:1009\u001b[0m, in \u001b[0;36mImage.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretina:\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retina_shape()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Dev-A_vYTw5b/lib/python3.11/site-packages/IPython/core/display.py:357\u001b[0m, in \u001b[0;36mDisplayObject.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_flags \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_flags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;66;03m# Deferred import\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'coding/nvidia_tesla_2024_ytd.png'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename=\"coding/nvidia_tesla_2024_ytd.png\")  # type: ignore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dev-A_vYTw5b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
