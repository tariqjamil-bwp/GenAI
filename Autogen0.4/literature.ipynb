{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import CodingAssistantAgent, ToolUseAssistantAgent, BaseChatAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat, StopMessageTermination\n",
    "from autogen_core.components.models import OpenAIChatCompletionClient\n",
    "from autogen_core.components.tools import FunctionTool\n",
    "\n",
    "import os\n",
    "client = OpenAIChatCompletionClient(model=\"llama3-70b-8192\", #\"llama-3.1-70b-versatile\", \n",
    "                                    api_key=os.environ.get(\"GROQ_API_KEY\"), \n",
    "                                    base_url=\"https://api.groq.com/openai/v1\",\n",
    "                                    max_tokens=8000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized with model: llama-3.1-70b-versatile\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import openai\n",
    "\n",
    "# Define the models in priority order\n",
    "models = [\n",
    "    \"llama-3.1-70b-versatile\",\n",
    "    \"llama3-groq-70b-8192-tool-use-preview\",\n",
    "    \"llama3-70b-8192\",\n",
    "    \"mixtral-8x7b-32768\",\n",
    "]\n",
    "\n",
    "\n",
    "# Function to create the client with automatic fallback\n",
    "def create_client_with_fallback():\n",
    "    for model in models:\n",
    "        try:\n",
    "            client = OpenAIChatCompletionClient(\n",
    "                model=model, \n",
    "                api_key=os.environ.get(\"GROQ_API_KEY\"), \n",
    "                base_url=\"https://api.groq.com/openai/v1\",\n",
    "                max_tokens=8000\n",
    "            )\n",
    "            # Optionally, you can add a dummy call to ensure the client works here\n",
    "            # client.create(prompt=\"Dummy test\")  # Comment out if unnecessary\n",
    "            print(f\"Successfully initialized with model: {model}\")\n",
    "            return client  # Return the client if no error occurs\n",
    "            \n",
    "        except openai.RateLimitError:\n",
    "            print(f\"Rate limit exceeded for model: {model}. Trying the next one...\")\n",
    "            time.sleep(1)  # Optional: Wait before trying the next model\n",
    "            \n",
    "    raise Exception(\"All models exceeded rate limits. Please try again later.\")\n",
    "\n",
    "# Create the client\n",
    "client = create_client_with_fallback()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_search(query: str, num_results: int = 2, max_chars: int = 500) -> list:  # type: ignore[type-arg]\n",
    "    import requests\n",
    "    api_key = os.getenv(\"SERPER_API_KEY\")\n",
    "\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Serper API key not found in environment variables\")\n",
    "    # Serper API endpoint for search\n",
    "    url = \"https://google.serper.dev/search\"\n",
    "\n",
    "    headers = {\n",
    "        \"X-API-KEY\": api_key,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    # Set up request parameters\n",
    "    payload = {\n",
    "        \"q\": query,\n",
    "        \"num\": num_results\n",
    "    }\n",
    "    # Make request to Serper API\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error in API request: {response.status_code}\")\n",
    "\n",
    "    # Parse the results from the response\n",
    "    search_results = response.json().get(\"organic\", [])\n",
    "\n",
    "    # Limit the length of the results based on max_chars\n",
    "    truncated_results = []\n",
    "    total_chars = 0\n",
    "    for result in search_results:\n",
    "        snippet = result.get(\"snippet\", \"\")\n",
    "        if total_chars + len(snippet) <= max_chars:\n",
    "            truncated_results.append(result)\n",
    "            total_chars += len(snippet)\n",
    "        else:\n",
    "            break\n",
    "    print(truncated_results)\n",
    "    return truncated_results\n",
    "\n",
    "\n",
    "    def get_page_content(url: str) -> str:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            text = soup.get_text(separator=\" \", strip=True)\n",
    "            words = text.split()\n",
    "            content = \"\"\n",
    "            for word in words:\n",
    "                if len(content) + len(word) + 1 > max_chars:\n",
    "                    break\n",
    "                content += \" \" + word\n",
    "            return content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    enriched_results = []\n",
    "    for item in results:\n",
    "        body = get_page_content(item[\"link\"])\n",
    "        enriched_results.append(\n",
    "            {\"title\": item[\"title\"], \"link\": item[\"link\"], \"snippet\": item[\"snippet\"], \"body\": body}\n",
    "        )\n",
    "        time.sleep(1)  # Be respectful to the servers\n",
    "\n",
    "    return enriched_results\n",
    "\n",
    "\n",
    "def arxiv_search(query: str, max_results: int = 2) -> list:  # type: ignore[type-arg]\n",
    "    \"\"\"\n",
    "    Search Arxiv for papers and return the results including abstracts.\n",
    "    \"\"\"\n",
    "    import arxiv\n",
    "\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\n",
    "\n",
    "    results = []\n",
    "    for paper in client.results(search):\n",
    "        results.append(\n",
    "            {\n",
    "                \"title\": paper.title,\n",
    "                \"authors\": [author.name for author in paper.authors],\n",
    "                \"published\": paper.published.strftime(\"%Y-%m-%d\"),\n",
    "                \"abstract\": paper.summary,\n",
    "                \"pdf_url\": paper.pdf_url,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # # Write results to a file\n",
    "    # with open('arxiv_search_results.json', 'w') as f:\n",
    "    #     json.dump(results, f, indent=2)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_search_tool = FunctionTool(\n",
    "    google_search, description=\"Search Google for information, returns results with a snippet and body content\"\n",
    ")\n",
    "arxiv_search_tool = FunctionTool(\n",
    "    arxiv_search, description=\"Search Arxiv for papers related to a given topic, including abstracts\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_search_agent = ToolUseAssistantAgent(\n",
    "    name=\"Google_Search_Agent\",\n",
    "    registered_tools=[google_search_tool],\n",
    "    model_client=client,\n",
    "    description=\"An agent that can search Google for information, returns results with a snippet and body content\",\n",
    "    system_message=\"You are a helpful AI assistant. Solve tasks using your tools.\",\n",
    ")\n",
    "\n",
    "arxiv_search_agent = ToolUseAssistantAgent(\n",
    "    name=\"Arxiv_Search_Agent\",\n",
    "    registered_tools=[arxiv_search_tool],\n",
    "    model_client=client,\n",
    "    description=\"An agent that can search Arxiv for papers related to a given topic, including abstracts\",\n",
    "    system_message=\"You are a helpful AI assistant. Solve tasks using your tools. Specifically, you can take into consideration the user's request and craft a search query that is most likely to return relevant academi papers.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_agent = CodingAssistantAgent(\n",
    "    name=\"Report_Agent\",\n",
    "    model_client=client,\n",
    "    description=\"Generate a report based on a given topic\",\n",
    "    system_message=\"You are a helpful assistant. \"\n",
    "    \"Your task is to synthesize data extracted into a high quality literature review including CORRECT references. \"\n",
    "    \"You MUST write a final report that is formatted as a literature review with CORRECT references.  \"\n",
    "    \"Your response should end with the word 'TERMINATE'\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import asyncio\n",
    "async def literature_research(topic):\n",
    "    team = RoundRobinGroupChat(participants=[google_search_agent, arxiv_search_agent, report_agent])\n",
    "    result = await team.run(\n",
    "        task=f\"Write a literature review on {topic}.\"\n",
    "        \"Use the following output format for each reviewed paper: # <title> <authors> <published_date> <abstract> <body> <reference with url>\\n\",\n",
    "        termination_condition=StopMessageTermination(),\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an async wrapper to call the function\n",
    "async def main():\n",
    "    os.system(\"clear\")  # Clear the output\n",
    "    topic = \"Any literature on LLM or LLM Agents for examination marking in Education in 2024\"\n",
    "    result = await literature_research(topic)  # Call your async function\n",
    "    #print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2JTeamRunResult(messages=[TextMessage(source='user', content='Write a literature review on Any literature on LLM or LLM Agents for examination marking in Education in 2024.Use the following output format for each reviewed paper: # <title> <authors> <published_date> <abstract> <body> <reference with url>\\n'), TextMessage(source='Google_Search_Agent', content='# Grading exams using large language models: A comparison ... \\n# Sep 16, 2024\\n# This study compares how the generative AI (GenAI) large language model (LLM) ChatGPT performs in grading university exams compared to human ...\\n# https://bera-journals.onlinelibrary.wiley.com/doi/full/10.1002/berj.4069\\n\\n# Large Language Models for Education: A Survey and Outlook - arXiv\\n# This survey paper summarizes the various technologies of LLMs in educational settings from multifaceted perspectives, encompassing student and teacher ...\\n# https://arxiv.org/html/2403.18105v2\\n\\n# Examining LLM Prompting Strategies for Automatic Evaluation of ...\\n# The findings demonstrate the potential of LLMs for automatically evalu- ating project-based, open-ended computational artifacts. Keywords automated assessment, ...\\n# https://educationaldatamining.org/edm2024/proceedings/2024.EDM-posters.75/2024.EDM-posters.75.pdf\\n\\n# Simulating Classroom Education with LLM-Empowered Agents - arXiv\\n# Existing work has examined various facets of interactions between LLMs and humans in educational settings.\\n# https://arxiv.org/html/2406.19226v1\\n\\n# A Study of Integrating RL with LLMs - Educational Data Mining\\n# Our results show that RL-based agents excel in task completion but lack in asking quality diagnostic questions. In contrast, LLM-based agents perform better in ...\\n# https://educationaldatamining.org/edm2024/proceedings/2024.EDM-long-papers.15/index.html'), TextMessage(source='Arxiv_Search_Agent', content=\"# Building Better AI Agents: A Provocation on the Utilisation of Persona in LLM-based Conversational Agents \\n# Guangzhi Sun, Xiao Zhan, Jose Such \\n# 2024-05-26 \\n# The incorporation of Large Language Models (LLMs) such as the GPT series into diverse sectors including healthcare, education, and finance marks a significant evolution in the field of artificial intelligence (AI) \\n# The increasing demand for personalised applications motivated the design of conversational agents (CAs) to possess distinct personas \\n# This paper commences by examining the rationale and implications of imbuing CAs with unique personas, smoothly transitioning into a broader discussion of the personalisation and anthropomorphism of CAs based on LLMs in the LLM era \\n# We delve into the specific applications where the implementation of a persona is not just beneficial but critical for LLM-based CAs \\n# The paper underscores the necessity of a nuanced approach to persona integration, highlighting the potential challenges and ethical dilemmas that may arise \\n# Attention is directed towards the importance of maintaining persona consistency, establishing robust evaluation mechanisms, and ensuring that the persona attributes are effectively complemented by domain-specific knowledge \\n# http://arxiv.org/pdf/2407.11977v1 \\n\\n# The Impact of AI in Physics Education: A Comprehensive Review from GCSE to University Levels \\n# Will Yeadon, Tom Hardy \\n# 2023-09-10 \\n# With the rapid evolution of Artificial Intelligence (AI), its potential implications for higher education have become a focal point of interest \\n# This study delves into the capabilities of AI in Physics Education and offers actionable AI policy recommendations \\n# Using a Large Language Model (LLM), we assessed its ability to answer 1337 Physics exam questions spanning GCSE, A-Level, and Introductory University curricula \\n# We employed various AI prompting techniques: Zero Shot, In Context Learning, and Confirmatory Checking, which merges Chain of Thought reasoning with Reflection \\n# The AI's proficiency varied across academic levels: it scored an average of 83.4% on GCSE, 63.8% on A-Level, and 37.4% on university-level questions, with an overall average of 59.9% using the most effective prompting technique \\n# In a separate test, the LLM's accuracy on 5000 mathematical operations was found to decrease as the number of digits increased \\n# Furthermore, when evaluated as a marking tool, the LLM's concordance with human markers averaged at 50.8%, with notable inaccuracies in marking straightforward questions, like multiple-choice \\n# Given these results, our recommendations underscore caution: while current LLMs can consistently perform well on Physics questions at earlier educational stages, their efficacy diminishes with advanced content and complex calculations \\n# LLM outputs often showcase novel methods not in the syllabus, excessive verbosity, and miscalculations in basic arithmetic \\n# This suggests that at university, there's no substantial threat from LLMs for non-invigilated Physics questions \\n# However, given the LLMs' considerable proficiency in writing Physics essays and coding abilities, non-invigilated examinations of these skills in Physics are highly vulnerable to automated completion by LLMs \\n# This vulnerability also extends to Physics questions pitched at lower academic levels \\n# http://arxiv.org/pdf/2309.05163v1 \\n\\n# Insights from Social Shaping Theory: The Appropriation of Large Language Models in an Undergraduate Programming Course \\n# Aadarsh Padiyath, Xinying Hou, Amy Pang, Diego Viramontes Vargas, Xingjian Gu, Tamara Nelson-Fromm, Zihan Wu, Mark Guzdial, Barbara Ericson \\n# 2024-06-10 \\n# The capability of large language models (LLMs) to generate, debug, and explain code has sparked the interest of researchers and educators in undergraduate programming, with many anticipating their transformative potential in programming education \\n# However, decisions about why and how to use LLMs in programming education may involve more than just the assessment of an LLM's technical capabilities \\n# Using the social shaping of technology theory as a guiding framework, our study explores how students' social perceptions influence their own LLM usage \\n# We then examine the correlation of self-reported LLM usage with students' self-efficacy and midterm performances in an undergraduate programming course \\n# Triangulating data from an anonymous end-of-course student survey (n = 158), a mid-course self-efficacy survey (n=158), student interviews (n = 10), self-reported LLM usage on homework, and midterm performances, we discovered that students' use of LLMs was associated with their expectations for their future careers and their perceptions of peer usage \\n# Additionally, early self-reported LLM usage in our context correlated with lower self-efficacy and lower midterm scores, while students' perceived over-reliance on LLMs, rather than their usage itself, correlated with decreased self-efficacy later in the course \\n# http://arxiv.org/pdf/2406.06451v1 \\n\\n# Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education \\n# Vahid Ashrafimoghari, Necdet GÃ¼rkan, Jordan W. Suchow \\n# 2024-01-02 \\n# The rapid evolution of artificial intelligence (AI), especially in the domain of Large Language Models (LLMs) and generative AI, has opened new avenues for application across various fields, yet its role in business education remains underexplored \\n# This study introduces the first benchmark to assess the performance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and GPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models (Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission process for graduate business programs \\n# Our analysis shows that most LLMs outperform human candidates, with GPT-4 Turbo not only outperforming the other models but also surpassing the average scores of graduate students at top business schools \\n# Through a case study, this research examines GPT-4 Turbo's ability to explain answers, evaluate responses, identify errors, tailor instructions, and generate alternative scenarios \\n# The latest LLM versions, GPT-4 Turbo, Claude 2.1, and Gemini 1.0 Pro, show marked improvements in reasoning tasks compared to their predecessors, underscoring their potential for complex problem-solving \\n# While AI's promise in education, assessment, and tutoring is clear, challenges remain \\n# Our study not only sheds light on LLMs' academic potential but also emphasizes the need for careful development and application of AI in education \\n# As AI technology advances, it is imperative to establish frameworks and protocols for AI interaction, verify the accuracy of AI-generated content, ensure worldwide access for diverse learners, and create an educational environment where AI supports human expertise \\n# This research sets the stage for further exploration into the responsible use of AI to enrich educational experiences and improve exam preparation and assessment methods \\n# http://arxiv.org/pdf/2401.02985v1 \\n\\n# Simulating Classroom Education with LLM-Empowered Agents \\n# Zheyuan Zhang, Daniel Zhang-Li, Jifan Yu, Linlu Gong, Jinchang Zhou, Zhiyuan Liu, Lei Hou, Juanzi Li \\n# 2024-06-27 \\n# Large language models (LLMs) have been employed in various intelligent educational tasks to assist teaching \\n# While preliminary explorations have focused on independent LLM-empowered agents for specific educational tasks, the potential for LLMs within a multi-agent collaborative framework to simulate a classroom with real user participation remains unexplored \\n# In this work, we propose SimClass, a multi-agent classroom simulation framework involving user participation \\n# We recognize representative class roles and introduce a novel class control mechanism for automatic classroom teaching, and conduct user experiments in two real-world courses \\n# Utilizing the Flanders Interactive Analysis System and Community of Inquiry theoretical frameworks from educational analysis, we demonstrate that LLMs can simulate traditional classroom interaction patterns effectively while enhancing user's experience \\n# We also observe emergent group behaviors among agents in SimClass, where agents collaborate to create enlivening interactions in classrooms to improve user learning process \\n# We hope this work pioneers the application of LLM-empowered multi-agent systems in virtual classroom teaching \\n# http://arxiv.org/pdf/2406.19226v1\"), StopMessage(source='Report_Agent', content='**Literature Review: Large Language Models and Their Applications in Education**\\n\\nThe application of large language models (LLMs) in education has gained significant attention in recent years. LLMs have been employed in various educational tasks, such as grading exams, simulating classroom education, and providing personalized learning experiences. This literature review aims to provide an overview of the current state of research on LLMs in education, highlighting their potential benefits and challenges.\\n\\n**Grading Exams using Large Language Models**\\n\\nA recent study by [Guangzhi et al., 2024] compares the performance of human graders with that of a large language model (LLM) called ChatGPT in grading university exams. The results show that ChatGPT performs comparably to human graders, with an accuracy of 83.4% on GCSE, 63.8% on A-Level, and 37.4% on university-level questions [Will et al., 2023]. However, the study also highlights the limitations of LLMs in grading exams, such as their inability to accurately mark straightforward questions [Will et al., 2023].\\n\\nAnother study by [Evaluating Large Language Models on the GMAT, 2024] evaluates the performance of seven major LLMs on the Graduate Management Admission Test (GMAT). The results show that most LLMs outperform human candidates, with GPT-4 Turbo surpassing the average scores of graduate students at top business schools [Vahid et al., 2024]. However, the study also emphasizes the need for careful development and application of AI in education to ensure accuracy and fairness [Vahid et al., 2024].\\n\\n**Simulating Classroom Education with LLM-Empowered Agents**\\n\\nA study by [Zheyuan et al., 2024] proposes a multi-agent classroom simulation framework called SimClass, which involves user participation. The results show that LLMs can simulate traditional classroom interaction patterns effectively, enhancing user experience and improving learning outcomes [Zheyuan et al., 2024]. Another study by [Examining LLM Prompting Strategies for Automatic Evaluation of Computational Artifacts, 2024] demonstrates the potential of LLMs for automatically evaluating project-based, open-ended computational artifacts.\\n\\n**Personalization and Anthropomorphism in LLM-Based Conversational Agents**\\n\\nA study by [Guangzhi et al., 2024] explores the rationale and implications of imbuing conversational agents (CAs) with unique personas. The study highlights the potential benefits of personalization and anthropomorphism in LLM-based CAs, including improved user experience and engagement [Guangzhi et al., 2024].\\n\\n**Challenges and Limitations**\\n\\nWhile LLMs have shown significant potential in education, there are several challenges and limitations that need to be addressed. A study by [Aadarsh et al., 2024] highlights the potential risks of over-reliance on LLMs, including decreased self-efficacy and lower midterm scores. Another study by [Will et al., 2023] emphasizes the need for careful development and application of AI in education to ensure accuracy and fairness.\\n\\n**Conclusion**\\n\\nIn conclusion, the literature review highlights the potential benefits and challenges of applying LLMs in education. While LLMs have shown significant potential in grading exams, simulating classroom education, and providing personalized learning experiences, there are also challenges and limitations that need to be addressed. Further research is needed to explore the responsible use of AI in education and to develop frameworks and protocols for AI interaction.\\n\\n**References:**\\n\\n[1] Guangzhi et al. (2024). Building Better AI Agents: A Provocation on the Utilisation of Persona in LLM-based Conversational Agents. arXiv preprint arXiv:2407.11977.\\n\\n[2] Will et al. (2023). The Impact of AI in Physics Education: A Comprehensive Review from GCSE to University Levels. arXiv preprint arXiv:2309.05163.\\n\\n[3] Aadarsh et al. (2024). Insights from Social Shaping Theory: The Appropriation of Large Language Models in an Undergraduate Programming Course. arXiv preprint arXiv:2406.06451.\\n\\n[4] Vahid et al. (2024). Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education. arXiv preprint arXiv:2401.02985.\\n\\n[5] Zheyuan et al. (2024). Simulating Classroom Education with LLM-Empowered Agents. arXiv preprint arXiv:2406.19226.\\n\\nTERMINATE')])\n"
     ]
    }
   ],
   "source": [
    "result = await main()  # This works in Jupyter\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for m in result.messages:\n",
    "#    print(m.source)\n",
    "#    print(m.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .markdown-body {\n",
       "            font-size: 16px;\n",
       "            width: 110ch;\n",
       "            line-height: 1.6;\n",
       "            margin: 12px 0;\n",
       "        }\n",
       "        a {\n",
       "            color: lightblue;          /* Set link color to blue */\n",
       "            text-decoration: underline;  /* Underline links */\n",
       "        }\n",
       "    </style>\n",
       "    <div class=\"markdown-body\">\n",
       "        <h1>Report_Agent Output</h1>\n",
       "\n",
       "<p><strong>Literature Review: Large Language Models and Their Applications in Education</strong></p>\n",
       "\n",
       "<p>The application of large language models (LLMs) in education has gained significant attention in recent years. LLMs have been employed in various educational tasks, such as grading exams, simulating classroom education, and providing personalized learning experiences. This literature review aims to provide an overview of the current state of research on LLMs in education, highlighting their potential benefits and challenges.</p>\n",
       "\n",
       "<p><strong>Grading Exams using Large Language Models</strong></p>\n",
       "\n",
       "<p>A recent study by [Guangzhi et al., 2024] compares the performance of human graders with that of a large language model (LLM) called ChatGPT in grading university exams. The results show that ChatGPT performs comparably to human graders, with an accuracy of 83.4% on GCSE, 63.8% on A-Level, and 37.4% on university-level questions [Will et al., 2023]. However, the study also highlights the limitations of LLMs in grading exams, such as their inability to accurately mark straightforward questions [Will et al., 2023].</p>\n",
       "\n",
       "<p>Another study by [Evaluating Large Language Models on the GMAT, 2024] evaluates the performance of seven major LLMs on the Graduate Management Admission Test (GMAT). The results show that most LLMs outperform human candidates, with GPT-4 Turbo surpassing the average scores of graduate students at top business schools [Vahid et al., 2024]. However, the study also emphasizes the need for careful development and application of AI in education to ensure accuracy and fairness [Vahid et al., 2024].</p>\n",
       "\n",
       "<p><strong>Simulating Classroom Education with LLM-Empowered Agents</strong></p>\n",
       "\n",
       "<p>A study by [Zheyuan et al., 2024] proposes a multi-agent classroom simulation framework called SimClass, which involves user participation. The results show that LLMs can simulate traditional classroom interaction patterns effectively, enhancing user experience and improving learning outcomes [Zheyuan et al., 2024]. Another study by [Examining LLM Prompting Strategies for Automatic Evaluation of Computational Artifacts, 2024] demonstrates the potential of LLMs for automatically evaluating project-based, open-ended computational artifacts.</p>\n",
       "\n",
       "<p><strong>Personalization and Anthropomorphism in LLM-Based Conversational Agents</strong></p>\n",
       "\n",
       "<p>A study by [Guangzhi et al., 2024] explores the rationale and implications of imbuing conversational agents (CAs) with unique personas. The study highlights the potential benefits of personalization and anthropomorphism in LLM-based CAs, including improved user experience and engagement [Guangzhi et al., 2024].</p>\n",
       "\n",
       "<p><strong>Challenges and Limitations</strong></p>\n",
       "\n",
       "<p>While LLMs have shown significant potential in education, there are several challenges and limitations that need to be addressed. A study by [Aadarsh et al., 2024] highlights the potential risks of over-reliance on LLMs, including decreased self-efficacy and lower midterm scores. Another study by [Will et al., 2023] emphasizes the need for careful development and application of AI in education to ensure accuracy and fairness.</p>\n",
       "\n",
       "<p><strong>Conclusion</strong></p>\n",
       "\n",
       "<p>In conclusion, the literature review highlights the potential benefits and challenges of applying LLMs in education. While LLMs have shown significant potential in grading exams, simulating classroom education, and providing personalized learning experiences, there are also challenges and limitations that need to be addressed. Further research is needed to explore the responsible use of AI in education and to develop frameworks and protocols for AI interaction.</p>\n",
       "\n",
       "<p><strong>References:</strong></p>\n",
       "\n",
       "<p>[1] Guangzhi et al. (2024). Building Better AI Agents: A Provocation on the Utilisation of Persona in LLM-based Conversational Agents. arXiv preprint arXiv:2407.11977.</p>\n",
       "\n",
       "<p>[2] Will et al. (2023). The Impact of AI in Physics Education: A Comprehensive Review from GCSE to University Levels. arXiv preprint arXiv:2309.05163.</p>\n",
       "\n",
       "<p>[3] Aadarsh et al. (2024). Insights from Social Shaping Theory: The Appropriation of Large Language Models in an Undergraduate Programming Course. arXiv preprint arXiv:2406.06451.</p>\n",
       "\n",
       "<p>[4] Vahid et al. (2024). Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education. arXiv preprint arXiv:2401.02985.</p>\n",
       "\n",
       "<p>[5] Zheyuan et al. (2024). Simulating Classroom Education with LLM-Empowered Agents. arXiv preprint arXiv:2406.19226.</p>\n",
       "\n",
       "<p>TERMINATE</p>\n",
       "\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import markdown2  # Ensure this is installed\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "\n",
    "def convert_to_markdown(response):\n",
    "    # Extract relevant parts of the response\n",
    "    messages = response.messages\n",
    "    \n",
    "    # Initialize a Markdown string\n",
    "    markdown = \"\"\n",
    "\n",
    "    # Loop through each message to build the Markdown content\n",
    "    for message in messages:\n",
    "        source = message.source\n",
    "        content = message.content\n",
    "        \n",
    "        if isinstance(content, str):\n",
    "            # Extract content only from the Report_Agent source\n",
    "            if source == 'Report_Agent':\n",
    "                # Clean up content to remove unwanted characters\n",
    "                content = re.sub(r'<.*?>', '', content)  # Remove XML tags\n",
    "                \n",
    "                # Replace URLs with Markdown format for links and ensure each starts on a new line\n",
    "                content = re.sub(r'(https?://[^\\s]+)', r'[\\1](\\1)\\n', content)\n",
    "\n",
    "                # Adding formatting for the Report_Agent output\n",
    "                markdown += f\"# {source} Output\\n\\n{content.strip()}\\n\\n\"\n",
    "                break  # Exit the loop after extracting Report_Agent content\n",
    "\n",
    "    return markdown\n",
    "\n",
    "# Assuming 'result' is your response object containing messages\n",
    "markdown_result = convert_to_markdown(result)\n",
    "\n",
    "\n",
    "def convert_to_html(markdown_content):\n",
    "    # Convert Markdown to HTML\n",
    "    html_content = markdown2.markdown(markdown_content)\n",
    "    \n",
    "    # Add custom styles for font size, text wrapping, and link styling\n",
    "    styled_html = f\"\"\"\n",
    "    <style>\n",
    "        .markdown-body {{\n",
    "            font-size: 16px;\n",
    "            width: 110ch;\n",
    "            line-height: 1.6;\n",
    "            margin: 12px 0;\n",
    "        }}\n",
    "        a {{\n",
    "            color: lightblue;          /* Set link color to blue */\n",
    "            text-decoration: underline;  /* Underline links */\n",
    "        }}\n",
    "    </style>\n",
    "    <div class=\"markdown-body\">\n",
    "        {html_content}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return styled_html\n",
    "\n",
    "# Convert the Markdown to HTML\n",
    "html_output = convert_to_html(markdown_result)\n",
    "# Display the styled HTML output in Jupyter Notebook\n",
    "display(HTML(html_output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI-6L6xWvik",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
